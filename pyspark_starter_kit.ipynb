{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Create DataFrame\").getOrCreate()\n",
    "df = spark.read.format(\"csv\").options(header='true', inferSchema='true', delimiter=',').load(\"path/to/file.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Specify the path to the folder containing Parquet files\n",
    "folder_path = \"/path/to/parquet/files/\"\n",
    "\n",
    "# Load all Parquet files in the folder into a DataFrame\n",
    "df = spark.read.parquet(folder_path)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.<storage-account-name>.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account-name>.dfs.core.windows.net\",  \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account-name>.dfs.core.windows.net\", \"<application-id>\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account-name>.dfs.core.windows.net\",\"<password>\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account-name>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing parquet file\n",
    "df.write.parquet(\"/path/to/output/parquet/file.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe based on a condition\n",
    "filtered_df = joined_df.filter(joined_df.column_name == 'desired_value')\n",
    "\n",
    "# Filter the dataframe based on multiple conditions\n",
    "filtered_df = joined_df.filter((joined_df.column_name1 > 10) & (joined_df.column_name2 == 'condition'))\n",
    "\n",
    "# Filter the dataframe based on null checks\n",
    "filtered_df = joined_df.filter(joined_df.column_name.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "agg_df = df.groupBy(df.age).agg(avg(df.salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy and perform an aggregation\n",
    "grouped_df = filtered_df.groupBy('column_name').agg({'aggregated_column': 'sum'})\n",
    "\n",
    "# Perform multiple aggregations\n",
    "grouped_df = filtered_df.groupBy('column_name').agg({'aggregated_column1': 'sum', 'aggregated_column2': 'avg'})\n",
    "\n",
    "# Apply filters after the GroupBy operation\n",
    "grouped_filtered_df = grouped_df.filter(grouped_df.aggregated_column > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Inner join combines two DataFrames based on the key (common column) provided and results in rows where there is a matching found. Rows from both DataFrames are dropped with a non-matching key.\n",
    "joined_df = df1.join(df2, on='common_column', how='inner')\n",
    "\n",
    "# It includes all rows from both DataFrames and fills in missing values with nulls where there is no match. In other words, it merges the DataFrames based on a common key, but retains all rows from both DataFrames, even if there’s no match.\n",
    "full_outer_join_df = df1.join(df2, join_condition, \"outer\")\n",
    "\n",
    "# returns all rows from the left dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found.\n",
    "left_outer_join_df = df1.join(df2, join_condition, \"left_outer\")\n",
    "\n",
    "# returns all rows from the right dataset regardless of math found on the left dataset, when join expression doesn’t match, it assigns null for that record and drops records from left where match not found.\n",
    "right_outer_join_df = df1.join(df2, join_condition, \"right_outer\")\n",
    "\n",
    "# returns only the rows from the left DataFrame (the first DataFrame mentioned in the join operation) where there is no match with the right DataFrame (the second DataFrame).\n",
    "left_anti_join_df = df1.join(df2, join_condition, \"left_anti\")\n",
    "\n",
    "# returns only the rows from the left DataFrame (the first DataFrame mentioned in the join operation) where there is a match with the right DataFrame (the second DataFrame).\n",
    "left_semi_join_df = df1.join(df2, join_condition, \"left_semi\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
